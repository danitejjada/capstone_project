{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import string \n",
    "from time import sleep #don't need this here \n",
    "from collections import Counter #don't need this here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords #don't need this here\n",
    "from nltk.stem import SnowballStemmer #don't need this here\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #don't need this here\n",
    "from sklearn.metrics.pairwise import cosine_similarity #don't need this here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_set = ['r','python','java','c++','ruby','perl','matlab','javascript','scala','excel','tableau',\n",
    "             'd3js','sas','spss','d3','hadoop','mapreduce','spark',\n",
    "             'pig','hive','shark','zookeeper','flume','mahout',\n",
    "             'sql','nosql','hase','cassandra','mongodb','docker','aws']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_split_join(arg):\n",
    "    '''\n",
    "    Input: job \n",
    "    Output: text formatted for indeed search \n",
    "    '''\n",
    "    arg = arg.split()\n",
    "    return '+'.join(word for word in arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_url(job): \n",
    "    \n",
    "    '''\n",
    "    Input: job \n",
    "    Output: url that directs to results page for the query \n",
    "    '''\n",
    "    job = url_split_join(job)\n",
    "    site_list = ['http://www.indeed.com/jobs?q=\"', job, '\"']      \n",
    "        \n",
    "    return ''.join(site_list) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_urls(job):\n",
    "    \n",
    "    '''\n",
    "    Goes through the result page for the query and return for urls for each organic job posting\n",
    "    Input: query\n",
    "    Output: list of all jobs urls features in the query \n",
    "    '''\n",
    "    \n",
    "    search_url = get_search_url(job) #gets results page \n",
    "    \n",
    "    try:\n",
    "        site = urlopen(search_url).read()\n",
    "    except:\n",
    "        return 'Invalid Search' #raises exception if search combination is invalid of if no jobs of that nature exist \n",
    "    \n",
    "    soup = BeautifulSoup(site)\n",
    "    \n",
    "    if len(soup) == 0: # in case the default parser lxml doesn't work, try another one\n",
    "        soup = BeautifulSoup(site, 'html5lib')\n",
    "    \n",
    "    \n",
    "   #gets the total number (organic and sponsored) of job postings\n",
    "    \n",
    "    num_jobs = soup.find(id = 'searchCount').string \n",
    "    num_jobs = re.findall('\\d+', num_jobs) \n",
    "    num_jobs = int(\"\".join(num_jobs[1:]))\n",
    "    \n",
    "    #gets the number of page results\n",
    "    if num_jobs > 10: \n",
    "        num_pages = num_jobs//10 \n",
    "    else: \n",
    "        num_pages = 1\n",
    "    \n",
    "    page_urls = [] \n",
    "    \n",
    "    #iterates over each page to get the urls within that page\n",
    "    for i in range(num_pages): \n",
    "\n",
    "        start_num = str(i*10)  #page 1 starts at start = 0 , page 1 starts at 10 etc.\n",
    "        page_url = ''.join([search_url,'&start=', start_num]) \n",
    "        \n",
    "        current_page = urlopen(page_url).read()\n",
    "        page_soup = BeautifulSoup(current_page)\n",
    "        \n",
    "        if len(page_soup) == 0: # In case the default parser lxml doesn't work, try another one\n",
    "            page_soup = BeautifulSoup(page_url, 'html5lib')\n",
    "        \n",
    "        results_col =  page_soup.find(id = 'resultsCol')  \n",
    "        organic_tags = results_col.find_all('div', {'data-tn-component' : \"organicJob\"}) #gets tags for organic rearch results \n",
    "         \n",
    "        urls  = [x.a.attrs.get('href') for x in organic_tags] #gets the url for the specific job \n",
    "        page_urls.append(urls)\n",
    "        \n",
    "        if len(urls) < 10: #necessary because sponsored jobs results included in num_jobs\n",
    "            break \n",
    "    \n",
    "    job_urls = ['https://www.indeed.com'+job for sublist in page_urls for job in sublist]\n",
    "    \n",
    "    return job_urls \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_info(job_url):\n",
    "    \n",
    "    '''\n",
    "    Input: url of indeed job posting\n",
    "    Output: role, title, location and list of words in description\n",
    "    '''\n",
    "    \n",
    "    #TO DO: 1) check that ds3 works \n",
    " \n",
    "    try:\n",
    "        site =  urlopen(job_url).read() #opens and returns html\n",
    "    except:\n",
    "        return \"url could not be opened and read\" #CHECK THIS \n",
    "    \n",
    "    soup = BeautifulSoup(site)\n",
    "    \n",
    "    if len(soup) == 0: # In case the default parser lxml doesn't work, try another one\n",
    "        soup = BeautifulSoup(site, 'html5lib')\n",
    "    \n",
    "    #general job information\n",
    "     \n",
    "    try:\n",
    "        job_title = soup.find('h3',{'class':\"icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title\"}).get_text()\n",
    "    except:\n",
    "        job_title = 'unavailable' \n",
    "    \n",
    "    try:\n",
    "        company_name = soup.find('div',{'class':'icl-u-lg-mr--sm icl-u-xs-mr--xs'}).get_text() #do the other company name thing, do this for state\n",
    "    except:\n",
    "        company_name = 'unavailable'\n",
    "\n",
    "    try:\n",
    "        company_info = soup.select('div.jobsearch-InlineCompanyRating.icl-u-xs-mt--xs.jobsearch-DesktopStickyContainer-companyrating')[0].text\n",
    "        company_info = company_info.split('-')[-1]\n",
    "        company_info = company_info.split(' ')\n",
    "    \n",
    "        info = []\n",
    "    \n",
    "        for i in company_info:\n",
    "            try:\n",
    "                i = int(i)\n",
    "                info.append(i)\n",
    "            except:\n",
    "                info.append(i)\n",
    "    \n",
    "        location = [x for x in info if not isinstance(x, int)]\n",
    "        city = ' '.join(location[:-1])\n",
    "        state = location[-1]\n",
    "    \n",
    "    except:\n",
    "        city = 'unavailable'\n",
    "        state = 'unavailable'\n",
    "       \n",
    "    try:\n",
    "        content = soup.find('div',{'class':'jobsearch-JobComponent-description icl-u-xs-mt--md'})\n",
    "        words = content.get_text().split()\n",
    "    \n",
    "        punctuation = string.punctuation\n",
    "        stop_words = stopwords.words('english')\n",
    "    \n",
    "        words =[''.join(ch for ch in word if ch not in punctuation) for word in words] #gets rid of punctuation between words to enable joint word adjustment and genereal punctuation \n",
    "        words = [re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\",word).split() for word in words] #adjusts for joint words\n",
    "    \n",
    "        words =[word.lower() for sublist in words for word in sublist] #flattens lists\n",
    "        job_description = [word for word in words if word not in stop_words and word not in punctuation] #gets rids of stop words\n",
    "\n",
    "    except:\n",
    "        job_description = 'Unavailable'\n",
    "    \n",
    "    return job_title, company_name, city[:-1], state, job_description\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_job_description(job_description,skill_set): \n",
    "    '''\n",
    "    Input: list of words included in job posting \n",
    "    Output: \n",
    "        Desc = list of stemmed words included in job posting no including tecnical skills \n",
    "        skills = technical skills required for job \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        skills = list(set([word for word in job_description if word in skill_set]))\n",
    "        text_no_skills = [word for word in job_description if word not in skills]\n",
    "    \n",
    "    except:\n",
    "        skills = 'unavailable'\n",
    "        text_no_skills = 'unavailable'\n",
    "    \n",
    "    return text_no_skills, skills \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(job, skill_set, city = None, state = None):\n",
    "    '''\n",
    "    Input:\n",
    "    Ouput: dataframe with revelant information about job\n",
    "        Words used for similiarty \n",
    "        Skills and location used for filetring results\n",
    "    '''\n",
    "    \n",
    "     #TO DO 1) how do i make this run faster \n",
    "    \n",
    "\n",
    "    job_urls = get_job_urls(job) \n",
    "    \n",
    "    if job_urls == 'Invalid Search':\n",
    "        return 'Invalid Search'\n",
    "    \n",
    "    job = []\n",
    "    unreadable_count = 0 \n",
    "    \n",
    "    for url in job_urls:\n",
    "        job_info = get_job_info(url)\n",
    "        \n",
    "        if job_info == \"url could not be opened and read\":\n",
    "            unreadable_count += 1\n",
    "        else:\n",
    "            job_title, company_name, city, state, job_description = job_info\n",
    "            text_no_skills, skills = clean_job_description(job_description,skill_set)\n",
    "            job.append({'job_title':job_title,\"company\":company_name,'city':city, 'state': state,'desc': text_no_skills, 'skills':skills,'url': url}) \n",
    "        \n",
    "    return pd.DataFrame(job), unreadable_count\n",
    "\n",
    "#here that dataframe needs to be with the constraints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_to_csv(job,file_name,skill_set,path):\n",
    "    '''\n",
    "    Input: job as typed out in indeed \n",
    "    '''\n",
    "    job_df, count = get_data(job,skill_set)\n",
    "    job_df['job'] = job\n",
    "    job_df.to_csv(path + file_name + '.csv')\n",
    "    \n",
    "    return count \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_count = dfs_to_csv('Data Scientist','data_scientist',skill_set,r'/Users/danielatejada 1/Desktop/Galvanize/capstone_project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mle_count = dfs_to_csv('Machine Learning Engineer', 'machine_learning_engineer',skill_set, r'/Users/danielatejada 1/Desktop/Galvanize/capstone_project/' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soft_developer_count = dfs_to_csv('Software Developer', 'softwate_developer',skill_set,r'/Users/danielatejada 1/Desktop/Galvanize/capstone_project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fsd_count = dfs_to_csv('Full Stack Developer', 'full_stack_developer',skill_set,r'/Users/danielatejada 1/Desktop/Galvanize/capstone_project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bia_count = dfs_to_csv('Business Intelligence Analyst','business_intelligence_analyst', skill_set,r'/Users/danielatejada 1/Desktop/Galvanize/capstone_project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_count = dfs_to_csv('Developer', 'developer', skill_set, r'/Users/danielatejada 1/Desktop/Galvanize/capstone_project/' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eng_count = dfs_to_csv(\"Data Engineer\", 'data_engineer', skill_set, r'/Users/danielatejada 1/Desktop/Galvanize/capstone_project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_arc_count = dfs_to_csv(\"Could Architect\", 'cloud_architect',skill_set,r'/Users/danielatejada 1/Desktop/Galvanize/capstone_project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arc_count = dfs_to_csv('Data Architect', 'data_architect',skill_set,r'/Users/danielatejada 1/Desktop/Galvanize/capstone_project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ops_count = dfs_to_csv('DevOps','dev_ops',skill_set, r'/Users/danielatejada 1/Desktop/Galvanize/capstone_project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "software_eng_count = dfs_to_csv('Sofwate Engineer','software_engineer',skill_set,r'/Users/danielatejada 1/Desktop/Galvanize/capstone_project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_url = get_job_urls('Data Scientist','Boston','MA')[0] #you have to deal with the .string thing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title, company_name, city, state, text_no_skills, skills = get_query_info(query_url,skill_set) #need to fix the title string thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sim(query_words, df):\n",
    "    #TO DO 1) here is where I need to incorporate semantic similiarty \n",
    "    #      2) can't stem prior to doing similiarty so need to unstem and then resteam\n",
    "     \n",
    "    query_words = [\" \".join(query)]\n",
    "    job_words = df.desc.apply(lambda x: \" \".join(x)).values.tolist() #this gives you a list of list of the jobs \n",
    "     \n",
    "    vectorizer = TfidfVectorizer() #make sure these defaults are correct\n",
    "    model = vectorizer.fit(job_words)\n",
    "    \n",
    "    query_tfidf = model.transform(query_words)\n",
    "    job_tfidf = model.transform(job_words)\n",
    "    \n",
    "    cosine_sim = cosine_similarity(query_tfidf,job_tfidf)\n",
    "    top_10_sim = cosine_sim.argsort()[0][::-1][0:10]\n",
    "    \n",
    "    return cosine_sim, top_10_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim, top_10_sim = get_top_sim(text_no_skills,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 22, 24, 13,  6, 23, 14,  8, 26, 18])"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim\n",
    "top_10_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_jobs(df,indices):\n",
    "    return df.iloc[indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>company</th>\n",
       "      <th>desc</th>\n",
       "      <th>job_title</th>\n",
       "      <th>skills</th>\n",
       "      <th>state</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Atlanta,</td>\n",
       "      <td>Epsilon</td>\n",
       "      <td>[sr, data, scientist, analyt, practic, respons...</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>[aws, python, spark, sql, hadoop, sas, java, s...</td>\n",
       "      <td>GA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=18fb5f0e261aa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Roswell,</td>\n",
       "      <td>MotoRad of America</td>\n",
       "      <td>[job, summari, respons, analyz, larg, amount, ...</td>\n",
       "      <td>Insights and Analytics Manager</td>\n",
       "      <td>[]</td>\n",
       "      <td>GA</td>\n",
       "      <td>https://www.indeed.com/company/MotoRad-of-Amer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Alpharetta,</td>\n",
       "      <td>ADP</td>\n",
       "      <td>[unlock, career, potenti, technolog, adp, enjo...</td>\n",
       "      <td>Full Stack Developer</td>\n",
       "      <td>[java, docker, excel]</td>\n",
       "      <td>GA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=37fafd1237590...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Atlanta,</td>\n",
       "      <td>Cox Communications</td>\n",
       "      <td>[descript, cox, communic, look, data, scientis...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[sql, python, spark, sas, java, scala, hive, r...</td>\n",
       "      <td>GA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=759ba16b59111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alpharetta,</td>\n",
       "      <td>Equifax</td>\n",
       "      <td>[job, number, 20765, posit, titl, extern, desc...</td>\n",
       "      <td>Data Scientist, Keying and Linking</td>\n",
       "      <td>[sql, r, sas]</td>\n",
       "      <td>GA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=337a80c92cae2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Atlanta,</td>\n",
       "      <td>Cox Communications</td>\n",
       "      <td>[descript, cox, communic, look, director, data...</td>\n",
       "      <td>Director, Data Science</td>\n",
       "      <td>[sql, python, spark, sas, java, scala, hive, r...</td>\n",
       "      <td>GA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=280ddd1362b17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Atlanta,</td>\n",
       "      <td>Catalina Marketing</td>\n",
       "      <td>[catalina, catalina, person, digit, media, con...</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>[python, r]</td>\n",
       "      <td>GA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=f7d028a809b84...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Atlanta,</td>\n",
       "      <td>Catalina Marketing</td>\n",
       "      <td>[catalina, catalina, person, digit, media, con...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[python, r]</td>\n",
       "      <td>GA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=303a953bd67d7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Alpharetta,</td>\n",
       "      <td>ADP</td>\n",
       "      <td>[adp, hire, princip, applic, develop, full, st...</td>\n",
       "      <td>Principal Application Developer</td>\n",
       "      <td>[java, docker, excel, sql]</td>\n",
       "      <td>GA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=63c3ab3e6804c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Seattle,</td>\n",
       "      <td>KPMG</td>\n",
       "      <td>[innov, collabor, shine, lighthous, –, kpmgs, ...</td>\n",
       "      <td>Sr. Associate, Data Scientist, NLP</td>\n",
       "      <td>[aws, python, sql]</td>\n",
       "      <td>WA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=062f6a71626aa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           city             company  \\\n",
       "20     Atlanta,             Epsilon   \n",
       "22     Roswell,  MotoRad of America   \n",
       "24  Alpharetta,                 ADP   \n",
       "13     Atlanta,  Cox Communications   \n",
       "6   Alpharetta,             Equifax   \n",
       "23     Atlanta,  Cox Communications   \n",
       "14     Atlanta,  Catalina Marketing   \n",
       "8      Atlanta,  Catalina Marketing   \n",
       "26  Alpharetta,                 ADP   \n",
       "18     Seattle,                KPMG   \n",
       "\n",
       "                                                 desc  \\\n",
       "20  [sr, data, scientist, analyt, practic, respons...   \n",
       "22  [job, summari, respons, analyz, larg, amount, ...   \n",
       "24  [unlock, career, potenti, technolog, adp, enjo...   \n",
       "13  [descript, cox, communic, look, data, scientis...   \n",
       "6   [job, number, 20765, posit, titl, extern, desc...   \n",
       "23  [descript, cox, communic, look, director, data...   \n",
       "14  [catalina, catalina, person, digit, media, con...   \n",
       "8   [catalina, catalina, person, digit, media, con...   \n",
       "26  [adp, hire, princip, applic, develop, full, st...   \n",
       "18  [innov, collabor, shine, lighthous, –, kpmgs, ...   \n",
       "\n",
       "                             job_title  \\\n",
       "20               Senior Data Scientist   \n",
       "22      Insights and Analytics Manager   \n",
       "24                Full Stack Developer   \n",
       "13                      Data Scientist   \n",
       "6   Data Scientist, Keying and Linking   \n",
       "23              Director, Data Science   \n",
       "14               Senior Data Scientist   \n",
       "8                       Data Scientist   \n",
       "26     Principal Application Developer   \n",
       "18  Sr. Associate, Data Scientist, NLP   \n",
       "\n",
       "                                               skills state  \\\n",
       "20  [aws, python, spark, sql, hadoop, sas, java, s...    GA   \n",
       "22                                                 []    GA   \n",
       "24                              [java, docker, excel]    GA   \n",
       "13  [sql, python, spark, sas, java, scala, hive, r...    GA   \n",
       "6                                       [sql, r, sas]    GA   \n",
       "23  [sql, python, spark, sas, java, scala, hive, r...    GA   \n",
       "14                                        [python, r]    GA   \n",
       "8                                         [python, r]    GA   \n",
       "26                         [java, docker, excel, sql]    GA   \n",
       "18                                 [aws, python, sql]    WA   \n",
       "\n",
       "                                                  url  \n",
       "20  https://www.indeed.com/rc/clk?jk=18fb5f0e261aa...  \n",
       "22  https://www.indeed.com/company/MotoRad-of-Amer...  \n",
       "24  https://www.indeed.com/rc/clk?jk=37fafd1237590...  \n",
       "13  https://www.indeed.com/rc/clk?jk=759ba16b59111...  \n",
       "6   https://www.indeed.com/rc/clk?jk=337a80c92cae2...  \n",
       "23  https://www.indeed.com/rc/clk?jk=280ddd1362b17...  \n",
       "14  https://www.indeed.com/rc/clk?jk=f7d028a809b84...  \n",
       "8   https://www.indeed.com/rc/clk?jk=303a953bd67d7...  \n",
       "26  https://www.indeed.com/rc/clk?jk=63c3ab3e6804c...  \n",
       "18  https://www.indeed.com/rc/clk?jk=062f6a71626aa...  "
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_jobs(data,top_10_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- how do you update relevant skills in case a new language comes up?\n",
    "    - word associated with data science\n",
    "    - essentailly i don't have to feed it in the skills_set\n",
    "- which state / city has job posting with the skills that you have\n",
    "    - If you're moving from one place to another what skills should you learn\n",
    "- On the website track the skills per day, per week, per month so you can see how skills are changing over time \n",
    "- Given a certain set of skill, return the job postings that for which you have the highest percentage of those skills\n",
    "\n",
    "\n",
    "## To Do \n",
    "1. Make sure that it is going through all the pages\n",
    "2. Make sure it is getting all the links on each page\n",
    "3. How do you want to organize\n",
    "4. Are the less jobs in of 84 than actual jobs\n",
    "5. How do you decrease run time \n",
    "6. Can I use this same webscrabing code to go through Udemy, Coursera, Udacity, Ed-Ex and determine which courses are the best?\n",
    "7. Have some sort of counter when the website term \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'http://www.indeed.com/jobs?q=%22', final_job, '%22&l=', final_city,\n",
    "                   '%2C+', state] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
